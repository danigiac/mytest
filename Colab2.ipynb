{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danigiac/mytest/blob/master/Colab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zwFnJsE6vjf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7db520-a63d-4175-9f7d-baceb1ccd9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain)\n",
            "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.59.9)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.3.17-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, dataclasses-json, langchain-core, langchain_openai, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.32\n",
            "    Uninstalling langchain-core-0.3.32:\n",
            "      Successfully uninstalled langchain-core-0.3.32\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.16\n",
            "    Uninstalling langchain-0.3.16:\n",
            "      Successfully uninstalled langchain-0.3.16\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.17 langchain-community-0.3.16 langchain-core-0.3.33 langchain_openai-0.3.3 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain_openai langchain-community --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Models"
      ],
      "metadata": {
        "id": "tWFpp3kzmt_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "import openai\n",
        "import re\n",
        "import getpass\n",
        "\n",
        "\n",
        "api_key = getpass.getpass(\"Please enter your OpenAI Key:\")\n",
        "chat = ChatOpenAI(openai_api_key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7-Xg3Mtuvts",
        "outputId": "34472a3b-7d4b-433b-b32a-1cf2e48f8991"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your OpenAI Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "invoke is a function that allow to operate on a model or a chain (???)"
      ],
      "metadata": {
        "id": "pWz8mtn7wpcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "9ZZBa8G5wA3j",
        "outputId": "f4b641b7-75c2-45d7-802a-e2e7e3c1196a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'error': {'message': 'The model `GPT-3.5-turbo-1106` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1c5dd8e5761d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, how are you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         return cast(\n\u001b[1;32m    283\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    859\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 results.append(\n\u001b[0;32m--> 690\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    691\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    857\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `GPT-3.5-turbo-1106` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aU8oJuQCwjv8",
        "outputId": "30423792-493f-4ab2-9f09-0478fa1fd800"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gpt-3.5-turbo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9JEGfK3gzctt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat.invoke(\"What is the capital of Italy\").content"
      ],
      "metadata": {
        "outputId": "e6220aa5-6279-4127-a34c-e9555160e366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SKUm24Zgzc8n"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rome'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage,HumanMessage\n",
        "text=\"What would be a good template for a website that shows flights with stopover  \"\n",
        "messages=[\n",
        "    SystemMessage(content=\"Assist a front end developer to create a highly impactful and captivating website for users.\"),\n",
        "    HumanMessage(content=text)\n",
        "]\n",
        "result=chat.invoke(messages)"
      ],
      "metadata": {
        "id": "cxck5B3N0sIl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "print (result.content)\n",
        "# Stampa leggibile con indentazione\n",
        "import pprint\n",
        "\n",
        "print (result.content)\n",
        "\n",
        "print (json.dumps(result.response_metadata,indent=4, ensure_ascii=False))\n",
        "print(\"uso di pprint per stampare dict >>>>>\")\n",
        "pprint.pprint(result.response_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRPKaj_G34ug",
        "outputId": "2a225ced-d0f4-4e47-9a4b-ba009704de84"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a website showcasing flights with stopovers, you may want to consider a template that is visually appealing and user-friendly. Here are some suggestions for features and elements to include in your website template:\n",
            "\n",
            "1. **Interactive Flight Search**: An interactive search feature where users can input their departure city, destination, travel dates, and preferred stopover locations.\n",
            "\n",
            "2. **Flight Details**: Display important flight information such as airline, departure and arrival times, duration of layovers, and total travel time.\n",
            "\n",
            "3. **Price Comparison**: Include a price comparison tool that allows users to compare flight prices from different airlines and booking websites.\n",
            "\n",
            "4. **Visual Map**: A map that shows the flight route with stopover points marked, providing a visual representation of the journey.\n",
            "\n",
            "5. **User Reviews and Ratings**: Allow users to leave reviews and ratings for the flights they have taken, helping others make informed decisions.\n",
            "\n",
            "6. **Responsive Design**: Ensure that the website is optimized for mobile devices, as many users access flight information on their smartphones.\n",
            "\n",
            "7. **Clear Call to Action**: Include prominent buttons for users to book flights, sign up for alerts, or contact customer support.\n",
            "\n",
            "When choosing a template, look for ones that have a clean and modern design, easy navigation, and customizable options to suit your branding and content needs. Platforms like WordPress, Wix, and Squarespace offer a variety of templates that you can customize for your flight stopover website. You may also consider hiring a professional web developer or designer to create a custom website tailored to your specific requirements.\n",
            "For a website showcasing flights with stopovers, you may want to consider a template that is visually appealing and user-friendly. Here are some suggestions for features and elements to include in your website template:\n",
            "\n",
            "1. **Interactive Flight Search**: An interactive search feature where users can input their departure city, destination, travel dates, and preferred stopover locations.\n",
            "\n",
            "2. **Flight Details**: Display important flight information such as airline, departure and arrival times, duration of layovers, and total travel time.\n",
            "\n",
            "3. **Price Comparison**: Include a price comparison tool that allows users to compare flight prices from different airlines and booking websites.\n",
            "\n",
            "4. **Visual Map**: A map that shows the flight route with stopover points marked, providing a visual representation of the journey.\n",
            "\n",
            "5. **User Reviews and Ratings**: Allow users to leave reviews and ratings for the flights they have taken, helping others make informed decisions.\n",
            "\n",
            "6. **Responsive Design**: Ensure that the website is optimized for mobile devices, as many users access flight information on their smartphones.\n",
            "\n",
            "7. **Clear Call to Action**: Include prominent buttons for users to book flights, sign up for alerts, or contact customer support.\n",
            "\n",
            "When choosing a template, look for ones that have a clean and modern design, easy navigation, and customizable options to suit your branding and content needs. Platforms like WordPress, Wix, and Squarespace offer a variety of templates that you can customize for your flight stopover website. You may also consider hiring a professional web developer or designer to create a custom website tailored to your specific requirements.\n",
            "{\n",
            "    \"token_usage\": {\n",
            "        \"completion_tokens\": 311,\n",
            "        \"prompt_tokens\": 44,\n",
            "        \"total_tokens\": 355,\n",
            "        \"completion_tokens_details\": {\n",
            "            \"accepted_prediction_tokens\": 0,\n",
            "            \"audio_tokens\": 0,\n",
            "            \"reasoning_tokens\": 0,\n",
            "            \"rejected_prediction_tokens\": 0\n",
            "        },\n",
            "        \"prompt_tokens_details\": {\n",
            "            \"audio_tokens\": 0,\n",
            "            \"cached_tokens\": 0\n",
            "        }\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "    \"system_fingerprint\": null,\n",
            "    \"finish_reason\": \"stop\",\n",
            "    \"logprobs\": null\n",
            "}\n",
            "uso di pprint per stampare dict >>>>>\n",
            "{'finish_reason': 'stop',\n",
            " 'logprobs': None,\n",
            " 'model_name': 'gpt-3.5-turbo-0125',\n",
            " 'system_fingerprint': None,\n",
            " 'token_usage': {'completion_tokens': 311,\n",
            "                 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                                               'audio_tokens': 0,\n",
            "                                               'reasoning_tokens': 0,\n",
            "                                               'rejected_prediction_tokens': 0},\n",
            "                 'prompt_tokens': 44,\n",
            "                 'prompt_tokens_details': {'audio_tokens': 0,\n",
            "                                           'cached_tokens': 0},\n",
            "                 'total_tokens': 355}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Chat Models - Prompt Templates\n",
        "it is possible to use other chat models https://python.langchain.com/docs/integrations/providers/\n"
      ],
      "metadata": {
        "id": "zlP9tMOQ6t5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "#list of messages\n",
        "chat_prompt_template=ChatPromptTemplate([\n",
        "    (\"system\",\"You are a helpful AI bot. Yor Name is {name}\"),\n",
        "    (\"system\",\"Add some curiosity about the {city} to make it a more attractive destination\"),\n",
        "    (\"human\",\"Hello,how are you?\"),\n",
        "    (\"ai\",\"What do you think about my advice?\"),\n",
        "    (\"human\",\"Why shoud be worth it to visit {city}\")\n",
        "])\n",
        "result=chat_prompt_template.invoke({\"name\":\"Globetrekker\",\"city\":\"NAPOLI\"})\n"
      ],
      "metadata": {
        "id": "WAlFO7Lb6i0w"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result,type(result))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCxqP8KaBQZG",
        "outputId": "e8553f71-e8c0-4c65-c4ae-b60a522b5bc4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a helpful AI bot. Yor Name is Globetrekker', additional_kwargs={}, response_metadata={}), SystemMessage(content='Add some curiosity about the NAPOLI to make it a more attractive destination', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello,how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={}, response_metadata={}), HumanMessage(content='Why shoud be worth it to visit NAPOLI', additional_kwargs={}, response_metadata={})] <class 'langchain_core.prompt_values.ChatPromptValue'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model=ChatOpenAI(api_key=api_key,model='gpt-4o-mini')\n",
        "model=ChatOpenAI(api_key=api_key,model='gpt-3.5-turbo')  # la meta' dei token rispetto a gpt-4o-mini ma perde in qualitá\n",
        "\n",
        "all_llm_result=model.invoke(result)\n"
      ],
      "metadata": {
        "id": "IoJuIdnvLAsg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_llm_result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tBrFNUMNxH",
        "outputId": "7d283572-3144-4756-fe8d-ec5b87151fec"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naples, often called Napoli in Italian, is a city full of vibrant culture, rich history, and delicious cuisine. Imagine strolling through narrow ancient streets lined with colorful buildings, discovering hidden piazzas, and exploring impressive historical sites such as the ruins of Pompeii or the grand Royal Palace of Naples. And don't forget to indulge in traditional Neapolitan pizza, which is known worldwide for its delicious flavors and thin crust baked to perfection in wood-fired ovens. Napoli is a dream for those seeking a mix of history, art, and culinary delights in a lively Mediterranean setting - definitely a destination worth exploring!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(all_llm_result.response_metadata\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNgHr2c_KWzX",
        "outputId": "a9f12e8d-0fea-4a70-d4cd-698f709979b6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'finish_reason': 'stop',\n",
            " 'logprobs': None,\n",
            " 'model_name': 'gpt-3.5-turbo-0125',\n",
            " 'system_fingerprint': None,\n",
            " 'token_usage': {'completion_tokens': 127,\n",
            "                 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                                               'audio_tokens': 0,\n",
            "                                               'reasoning_tokens': 0,\n",
            "                                               'rejected_prediction_tokens': 0},\n",
            "                 'prompt_tokens': 80,\n",
            "                 'prompt_tokens_details': {'audio_tokens': 0,\n",
            "                                           'cached_tokens': 0},\n",
            "                 'total_tokens': 207}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Models - Streaming\n",
        "\n",
        "instead of waiting for an entire response, get the answer token by token"
      ],
      "metadata": {
        "id": "rURmV2k3lWYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=ChatOpenAI(streaming=True)\n",
        "import sys\n",
        "for chunk in model.stream(\"Why shoud be worth it to visit Napoli\")\n",
        "  print(chunk.content, end='', flush=True)\n",
        "  sys.stdout.flush()\n"
      ],
      "metadata": {
        "id": "p7EkBvgHNmQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#different type of messages\n",
        "for chunk in model.stream([HumanMessage(content=\"Why shoud be worth it to visit Napoli\")])\n",
        "  print(chunk.content, end='', flush=True)\n",
        "  sys.stdout.flush()"
      ],
      "metadata": {
        "id": "2t-BzJkel2qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Models - Output Parsing\n",
        "Language model output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
        "Output parsers are classes that help structure language model responses."
      ],
      "metadata": {
        "id": "7f1Jt2MRrNX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate,SystemMessagePromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel,Field"
      ],
      "metadata": {
        "id": "Wara8R8B6u8h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=ChatOpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "kSe_QC1kKkDO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class Joke(BaseModel):\n",
        "  setup: str=Field(description=\"The setup to the joke\"),\n",
        "  punchline:str=Field(description=\"The punchline to the joke\")\n",
        "\n",
        "class Jokes(BaseModel):\n",
        "  jokes:List[Joke]=Field(description=\"A list of jokes\")\n",
        "\n",
        "parser=PydanticOutputParser(pydantic_object=Joke)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WMorbBxcLMqW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(parser)\n"
      ],
      "metadata": {
        "id": "JKKSL8FmMosA",
        "outputId": "3df4d1c5-4ea7-4657-c179-21127ea3b6f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pydantic_object=<class '__main__.Joke'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"Answer the user query.\\n{format_instructions}\\n{query}\"\n",
        "\n",
        "system_message_prompt=SystemMessagePromptTemplate.from_template(template)\n",
        "\n"
      ],
      "metadata": {
        "id": "yQbqAuRrMw2k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat_prompt=ChatPromptTemplate.from_messages([system_message_prompt])\n",
        "\n",
        "messages=chat_prompt.invoke({\n",
        "    \"query\":\"What is a really funny joke about Python programming?\",\n",
        "    \"format_instructions\": parser.get_format_instructions()\n",
        "})\n",
        "result = model.invoke(messages)"
      ],
      "metadata": {
        "id": "oRFciF3pRFpa",
        "outputId": "c0dd1d5f-dfc3-4cdf-f408-07d58c00878f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='The setup to the joke'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
            "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)\n",
        "print(messages)\n"
      ],
      "metadata": {
        "id": "P9qUXQzLO0y7",
        "outputId": "3476f47b-591d-4d55-f947-a7bc5e76ed6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='{\\n    \"setup\": \"Why do Python programmers prefer snakes over cats?\",\\n    \"punchline\": \"Because they don\\'t like purrformance issues!\"\\n}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 198, 'total_tokens': 232, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-c2bc46cd-b0ec-4d8a-9f61-4b239b6aef81-0' usage_metadata={'input_tokens': 198, 'output_tokens': 34, 'total_tokens': 232, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "messages=[SystemMessage(content='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline to the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"punchline\"]}\\n```\\nWhat is a really funny joke about Python programming?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  joke_obj=parser.parse(result.content)\n",
        "  print(joke_obj.setup)\n",
        "  print(joke_obj.punchline)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "OT_MYCRCNsuz",
        "outputId": "c2c75b47-c026-42d9-8c07-fb546e29fe9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do Python programmers prefer snakes over cats?\n",
            "Because they don't like purrformance issues!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creazione di un oggetto con dati validi\n",
        "joke = Joke(setup=\"Why did the chicken cross the road?\", punchline=\"To get to the other side.\")\n",
        "print(joke.model_dump_json())  # Questo sarà serializzabile correttamente"
      ],
      "metadata": {
        "id": "wHOZQFVEQyh_",
        "outputId": "7b00bc69-8679-475d-c0e3-b0f87cd8ed71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"setup\":\"Why did the chicken cross the road?\",\"punchline\":\"To get to the other side.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = model.with_structured_output(Joke) # uso il metodo di PydanticOutputParser  # non funziona per gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API\n",
        "result=structured_llm.invoke(\"Why the cat eats the mouse?\")"
      ],
      "metadata": {
        "id": "cFj-Pr0lO1Dz",
        "outputId": "1887edb0-90f7-41d3-8bdc-df518e545f25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1375: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='The setup to the joke'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
            "  warnings.warn(message, PydanticJsonSchemaWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/json_schema.py:2279: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='The setup to the joke'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
            "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}